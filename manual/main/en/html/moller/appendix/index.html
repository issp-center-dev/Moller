<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6. Extension guide &#8212; Moller Users Guide 1.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css?v=94bc0932" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=fd3f3429" />
    <script src="../../_static/documentation_options.js?v=292eb321"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="5. File format" href="../filespec/index.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="extension-guide">
<h1><span class="section-number">6. </span>Extension guide<a class="headerlink" href="#extension-guide" title="Link to this heading">¶</a></h1>
<p>N.B. The content of this section may vary depending on the version of <em>moller</em>.</p>
<section id="bulk-job-execution-by-moller">
<h2><span class="section-number">6.1. </span>Bulk job execution by <em>moller</em><a class="headerlink" href="#bulk-job-execution-by-moller" title="Link to this heading">¶</a></h2>
<p>A bulk job execution means that a set of small tasks are executed in parallel within a single batch job submitted to a large batch queue. It is schematically shown as follows, in which N tasks are launched as background processes and executed in parallel, and a <code class="docutils literal notranslate"><span class="pre">wait</span></code> statement is invoked to wait for all tasks to be completed.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>task<span class="w"> </span>param_1<span class="w"> </span><span class="p">&amp;</span>
task<span class="w"> </span>param_2<span class="w"> </span><span class="p">&amp;</span>
<span class="w">     </span>...
task<span class="w"> </span>param_N<span class="w"> </span><span class="p">&amp;</span>
<span class="nb">wait</span>
</pre></div>
</div>
<p>To manage the bulk job, it is required to distribute nodes and cores allocated to the batch job over the tasks param_1 … param_N so that they are executed on distinct nodes and cores. It is also needed to arrange task execution where at most N tasks are run simultaneously according to the allocated resources.</p>
<p>Hereafter a job script generated by <em>moller</em> will be denoted as a moller script.
In a moller script, the concurrent execution and control of tasks are managed by GNU parallel [1]. It takes a list holding the items param_1 … param_N and runs commands for each items in parallel. An example is given as follows, where list.dat contains param_1 … param_N in each line.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span>list.dat<span class="w"> </span><span class="p">|</span><span class="w"> </span>parallel<span class="w"> </span>-j<span class="w"> </span>N<span class="w"> </span>task
</pre></div>
</div>
<p>The number of concurrent tasks is determined at runtime from the number of nodes and cores obtained from the execution environment and the degree of parallelism (number of nodes, processes, and threads specified by node parameter).</p>
<p>The way to assign tasks to nodes and cores varies according to the job scheduler.
For SLURM job scheduler variants, the concurrent calls of <code class="docutils literal notranslate"><span class="pre">srun</span></code> command within the batch job are appropriately assigned to the nodes and cores by exploiting the option of exclusive resource usage. The explicit option may depend on the platform.</p>
<p>On the other hand, for PBS job scheduler variants that do not have such features, the distribution of nodes and cores to tasks has to be handled within the moller script. The nodes and cores allocated to a batch job are divided into <em>slots</em>, and the slots are assigned to the concurrent tasks. The division is determined from the allocated nodes and cores and the degree of parallelism of the task, and kept in a form of table variables. Within a task, the programs are executed on the assigned hosts and cores (optionally pinned to the program) through the options to mpirun (or mpiexec) and the environment variables. This feature depends on the MPI implementation.</p>
<p><strong>Reference</strong></p>
<p>[1] <a class="reference external" href="https://www.usenix.org/publications/login/february-2011-volume-36-number-1/gnu-parallel-command-line-power-tool">O. Tange, GNU Parallel - The command-Line Power Tool, ;login: The USENIX Magazine, February 2011:42-47.</a></p>
</section>
<section id="how-moller-works">
<h2><span class="section-number">6.2. </span>How <em>moller</em> works<a class="headerlink" href="#how-moller-works" title="Link to this heading">¶</a></h2>
<section id="structure-of-moller-script">
<h3>Structure of moller script<a class="headerlink" href="#structure-of-moller-script" title="Link to this heading">¶</a></h3>
<p><em>moller</em> reads the input YAML file and generates a job script for bulk execution. The structure of the generated script is described as follows.</p>
<ol class="arabic">
<li><p>Header</p>
<p>This part contains the options to the job scheduler. The content of the platform section is formatted according to the type of job scheduler. This feature depends on platforms.</p>
</li>
<li><p>Prologue</p>
<p>This part corresponds to the prologue section of the input file. The content of the <code class="docutils literal notranslate"><span class="pre">code</span></code> block is written as-is.</p>
</li>
<li><p>Function definitions</p>
<p>This part contains the definitions of functions and variables used within the moller script. The description of the functions will be given in the next section. This feature depends on platforms.</p>
</li>
<li><p>Processing Command-line options</p>
<p>The SLURM variants accept additional arguments to the job submission command (sbatch) that are passed to the job script as a command-line options. The name of the list file and/or the options such as the retry feature can be processed.</p>
<p>For the PBS variants, these command-line arguments are ignored, and therefore the name of the list file is fixed to <code class="docutils literal notranslate"><span class="pre">list.dat</span></code> by default, and the retry feature may be enabled by modifying the script with <code class="docutils literal notranslate"><span class="pre">retry</span></code> set to 1.</p>
</li>
<li><p>Description of tasks</p>
<p>This part contains the description of tasks specified in the jobs section of the input file. When more than one task is given, the following procedure is applied to each task.</p>
<blockquote>
<div><p>When parallel = false, the content of the <code class="docutils literal notranslate"><span class="pre">run</span></code> block is written as-is.</p>
<p>When parallel = true (default), a function is created by the name task_{task name} that contains the pre-processing for concurrent execution and the content of the <code class="docutils literal notranslate"><span class="pre">run</span></code> block. The keywords for the parallel execution (<code class="docutils literal notranslate"><span class="pre">srun</span></code>, <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code>, or <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>) are substituted by the platform-dependent command. The definition of the task function is followed by the concurrent execution command.</p>
</div></blockquote>
</li>
<li><p>Epilogue</p>
<p>This part corresponds to the epilogue section of the input file. The content of the <code class="docutils literal notranslate"><span class="pre">code</span></code> block is written as-is.</p>
</li>
</ol>
</section>
<section id="brief-description-of-moller-script-functions">
<h3>Brief description of moller script functions<a class="headerlink" href="#brief-description-of-moller-script-functions" title="Link to this heading">¶</a></h3>
<p>The main functions of the moller script is briefly described below.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">run_parallel</span></code></p>
<p>This function performs concurrent execution of task functions. It takes the degree of parallelism, the task function, and the status file as arguments. Within the function, it calls <code class="docutils literal notranslate"><span class="pre">_find_multiplicity</span></code> to find the number of tasks that can be run simultaneously, and invokes GNU parallel to run tasks concurrently. The task function is actually wrapped by the <code class="docutils literal notranslate"><span class="pre">_run_parallel_task</span></code> function to deal with the nested call of GNU parallel.</p>
<p>The platform-dependence is separated out by the functions <code class="docutils literal notranslate"><span class="pre">_find_multiplicity</span></code> and <code class="docutils literal notranslate"><span class="pre">_setup_run_parallel</span></code>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">_find_multiplicity</span></code></p>
<p>This function determines the number of tasks that can be simultaneously executed on the allocated resources (nodes and cores) taking account of the degree of parallelism of the task.
For the PBS variants, the compute nodes and the cores are divided into slots, and the slots are kept as table variables.
The information obtained at the batch job execution is summarized as follows.</p>
<ul>
<li><p>For SLURM variants,</p>
<blockquote>
<div><p>The number of allocated nodes (<code class="docutils literal notranslate"><span class="pre">_nnodes</span></code>)</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">SLURM_NNODES</span></code></p>
</div></blockquote>
<p>The number of allocated cores (<code class="docutils literal notranslate"><span class="pre">_ncores</span></code>)</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">SLURM_CPUS_ON_NODE</span></code></p>
</div></blockquote>
</div></blockquote>
</li>
<li><p>For PBS variants,</p>
<blockquote>
<div><p>The allocated nodes (<code class="docutils literal notranslate"><span class="pre">_nodes[]</span></code>)</p>
<blockquote>
<div><p>The list of unique compute nodes is obtained from the file given by <code class="docutils literal notranslate"><span class="pre">PBS_NODEFILE</span></code>.</p>
</div></blockquote>
<p>The number of allocated nodes (<code class="docutils literal notranslate"><span class="pre">_nnodes</span></code>)</p>
<blockquote>
<div><p>The number of entries of <code class="docutils literal notranslate"><span class="pre">_nodes[]</span></code>.</p>
</div></blockquote>
<p>The number of allocated cores</p>
<blockquote>
<div><p>Searched from below (in order of examination)</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NCPUS</span></code> (for PBS Professional)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">core</span></code> parameter of platform section (written in the script as a variable <code class="docutils literal notranslate"><span class="pre">moller_core</span></code>.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ncpus</span></code> or <code class="docutils literal notranslate"><span class="pre">ppn</span></code> parameter in the header.</p></li>
</ul>
</div></blockquote>
</div></blockquote>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">_setup_run_parallel</span></code></p>
<p>This function is called from the <code class="docutils literal notranslate"><span class="pre">run_parallel</span></code> function to supplement some procedures before running GNU parallel.
For PBS variants, the slot tables are exported so that the task functions can refer to.
For SLURM variants, there is nothing to do.</p>
</li>
</ul>
<p>The structure of the task function is described as follows.</p>
<ul class="simple">
<li><p>A task function is created by a name <code class="docutils literal notranslate"><span class="pre">task_{task</span> <span class="pre">name}</span></code>.</p></li>
<li><p>The arguments of the task function are 1) the degree of parallelism (the number of nodes, processes, and threads), 2) the execution directory (that corresponds to the entry of list file), 3) the slot ID assigned by GNU parallel.</p></li>
<li><p>The platform-dependent <code class="docutils literal notranslate"><span class="pre">_setup_taskenv</span></code> function is called to set up execution environment.
For PBS variants, the compute node and the cores are obtained from the slot table based on the slot ID. For SLURM variants, there is nothing to do.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">_is_ready</span></code> function is called to check if the preceding task has been completed successfully. If it is true, the remaining part of the function is executed. Otherwise, the task is terminated with the status -1.</p></li>
<li><p>The content of the <code class="docutils literal notranslate"><span class="pre">code</span></code> block is written. The keywords for parallel calculation (<code class="docutils literal notranslate"><span class="pre">srun</span></code>, <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code>, or <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>) are substituted by the command provided for the platform.</p></li>
</ul>
</section>
</section>
<section id="how-to-extend-moller-for-other-systems">
<h2><span class="section-number">6.3. </span>How to extend <em>moller</em> for other systems<a class="headerlink" href="#how-to-extend-moller-for-other-systems" title="Link to this heading">¶</a></h2>
<p>The latest version of <em>moller</em> provides profiles for ISSP supercomputer systems, ohtaka and kugui. An extension guide to use <em>moller</em> in other systems is described in the following.</p>
<section id="class-structure">
<h3>Class structure<a class="headerlink" href="#class-structure" title="Link to this heading">¶</a></h3>
<p>The platform-dependent parts of <em>moller</em> are placed in the directory <code class="docutils literal notranslate"><span class="pre">platform/</span></code>.
Their class structure is depicted below.</p>
<div class="graphviz"><img src="../../_images/graphviz-06368183531b8fc81c8d91cef6b17e1dc3c86493.png" alt="digraph class_diagram {
size=&quot;5,5&quot;
node[shape=record,style=filled,fillcolor=gray95]
edge[dir=back,arrowtail=empty]

Platform[label=&quot;{Platform (base.py)}&quot;]
BaseSlurm[label=&quot;{BaseSlurm (base_slurm.py)}&quot;]
BasePBS[label=&quot;{BasePBS (base_pbs.py)}&quot;]
BaseDefault[label=&quot;{BaseDefault (base_default.py)}&quot;]

Ohtaka[label=&quot;{Ohtaka (ohtaka.py)}&quot;]
Kugui[label=&quot;{Kugui (kugui.py)}&quot;]
Pbs[label=&quot;{Pbs (pbs.py)}&quot;]
Default[label=&quot;{DefaultPlatform (default.py)}&quot;]

Platform-&gt;BaseSlurm
Platform-&gt;BasePBS
Platform-&gt;BaseDefault

BaseSlurm-&gt;Ohtaka
BasePBS-&gt;Kugui
BasePBS-&gt;Pbs
BaseDefault-&gt;Default
}" class="graphviz" /></div>
<p>A factory is provided to select a system in the input file.
A class is imported in <code class="docutils literal notranslate"><span class="pre">platform/__init__.py</span></code> and registered to the factory by <code class="docutils literal notranslate"><span class="pre">register_platform(system_name,</span> <span class="pre">class_name)</span></code>, and then it becomes available in the system parameter of the platform section in the input YAML file.</p>
</section>
<section id="slurm-job-scheduler-variants">
<h3>SLURM job scheduler variants<a class="headerlink" href="#slurm-job-scheduler-variants" title="Link to this heading">¶</a></h3>
<p>For the SLURM job scheduler variants, the system-specific settings should be applied to a derived class of BaseSlurm class.
The string that substitute the keywords for the parallel execution of programs is given by the return value of <code class="docutils literal notranslate"><span class="pre">parallel_command()</span></code> method. It corresponds to the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command with the options for the exclusive use of resources. See ohtaka.py for an example.</p>
</section>
<section id="pbs-job-scheduler-variants">
<h3>PBS job scheduler variants<a class="headerlink" href="#pbs-job-scheduler-variants" title="Link to this heading">¶</a></h3>
<p>For the PBS job scheduler variants (PBS Professional, OpenPBS, Torque, and others), the system-specific settings should be applied to a derived class of BasePBS class.</p>
<p>There are two ways of specifying the number of nodes for a batch job in the PBS variants. PBS Professional takes the form of select=N:ncpus=n, while Torque and others take the form of node=N:ppn=n. The BasePBS class has a parameter <code class="docutils literal notranslate"><span class="pre">self.pbs_use_old_format</span></code> that is set to <code class="docutils literal notranslate"><span class="pre">True</span></code> for the latter type.</p>
<p>The number of cores per compute node can be specified by node parameter of the input file, while the default value may be set for a known system. In kugui.py, the number of cores per node is set to 128 by default.</p>
</section>
<section id="customizing-features">
<h3>Customizing features<a class="headerlink" href="#customizing-features" title="Link to this heading">¶</a></h3>
<p>When further customization is required, the methods of the base class may be overridden in the derived classes. The list of relevant methods is given below.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">setup</span></code></p>
<p>This method extracts parameters of the platform section.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">parallel_command</span></code></p>
<p>This method returns a string that is used to substitute the keywords for parallel execution of programs (<code class="docutils literal notranslate"><span class="pre">srun</span></code>, <code class="docutils literal notranslate"><span class="pre">mpiexec</span></code>, <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>).</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">generate_header</span></code></p>
<p>This method generates the header part of the job script that contains options to the job scheduler.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">generate_function</span></code></p>
<p>This method generates functions that are used within the moller script. It calls the following methods to generate function body and variable definitions.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">generate_variable</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">generate_function_body</span></code></p></li>
</ul>
<p>The definitions of the functions are provided as embedded strings within the class.</p>
</li>
</ul>
</section>
<section id="porting-to-new-type-of-job-scheduler">
<h3>Porting to new type of job scheduler<a class="headerlink" href="#porting-to-new-type-of-job-scheduler" title="Link to this heading">¶</a></h3>
<p>The platform-dependent parts of the moller scripts are the calculation of task multiplicity, the resource distribution over tasks, and the command string of parallel calculation.
The internal functions need to be developed with the following information on the platform:</p>
<ul class="simple">
<li><p>how to acquire the allocated nodes and cores from the environment at the execution of batch jobs.</p></li>
<li><p>how to launch parallel calculation (e.g. mpiexec command) and how to assign the nodes and cores to the command.</p></li>
</ul>
<p>To find which environment variables are set within the batch jobs, it may be useful to call <code class="docutils literal notranslate"><span class="pre">printenv</span></code> command in the job script.</p>
</section>
<section id="trouble-shooting">
<h3>Trouble shooting<a class="headerlink" href="#trouble-shooting" title="Link to this heading">¶</a></h3>
<p>When the variable <code class="docutils literal notranslate"><span class="pre">_debug</span></code> in the moller script is set to 1, the debug outputs are printed during the execution of the batch jobs. If the job does not work well, it is recommended that the debug option is turned on and the output is examined to check if the internal parameters are appropriately defined.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">Moller Users Guide</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Comprehensive Calculation Utility (moller)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../about/index.html">1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basic-usage.html">2. Installation and basic usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial/index.html">3. Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="../command/index.html">4. Command reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../filespec/index.html">5. File format</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">6. Extension guide</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Comprehensive Calculation Utility (moller)</a><ul>
      <li>Previous: <a href="../filespec/index.html" title="previous chapter"><span class="section-number">5. </span>File format</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2023-, The University of Tokyo.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.4.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="../../_sources/moller/appendix/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>